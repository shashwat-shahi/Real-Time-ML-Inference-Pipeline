# Real-Time ML Inference Pipeline

A high-performance, deterministic inference system with microsecond-level latency guarantees for computer vision models. Built with lock-free algorithms, real-time memory management, and custom scheduling policies to achieve <100Î¼s P99 latency.

## Features

### ðŸš€ Ultra-Low Latency
- **<100Î¼s P99 latency** for computer vision inference
- Deterministic execution timing with real-time guarantees
- Lock-free data structures for contention-free operation

### ðŸ”§ Real-Time Systems Engineering
- **Lock-free SPSC queues** for task scheduling
- **Real-time memory pools** with deterministic allocation
- **Custom thread scheduler** with priority-based execution
- **CPU affinity control** and real-time thread configuration

### âš¡ High-Performance Computing
- **CUDA-accelerated inference** for GPU computation
- **Zero-copy memory operations** where possible
- **Cache-aligned data structures** for optimal performance
- **Vectorized operations** and SIMD optimizations

### ðŸ“Š Performance Monitoring
- **Comprehensive latency tracking** with percentile analysis
- **Real-time performance metrics** collection
- **Deadline violation monitoring**
- **Throughput and success rate tracking**

## Technology Stack

- **C++17** - Modern C++ with real-time programming patterns
- **CUDA** - GPU acceleration for inference workloads
- **CMake** - Cross-platform build system
- **Lock-free Programming** - Wait-free data structures
- **Real-time Systems** - Deterministic timing guarantees

## Quick Start

### Prerequisites

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install cmake build-essential nvidia-cuda-toolkit

# CentOS/RHEL
sudo yum install cmake gcc-c++ cuda-toolkit
```

### Building

```bash
git clone https://github.com/shashwat-shahi/Real-Time-ML-Inference-Pipeline.git
cd Real-Time-ML-Inference-Pipeline
mkdir build && cd build
cmake ..
make -j$(nproc)
```

### Running

```bash
# Run the main demo
./realtime_inference

# Run tests
make test

# Run benchmarks
./benchmarks/latency_benchmark
./benchmarks/throughput_benchmark
```

## Architecture

### Core Components

1. **Lock-Free Queue** (`lockfree_queue.hpp`)
   - Single-producer, single-consumer queue
   - Cache-aligned atomic operations
   - Power-of-2 capacity for efficient modulo operations

2. **Memory Pool** (`memory_pool.hpp`)
   - Pre-allocated fixed-size blocks
   - Deterministic allocation/deallocation
   - Thread-safe with atomic operations

3. **Real-Time Scheduler** (`scheduler.hpp`)
   - Priority-based task scheduling
   - Deadline-aware execution
   - CPU affinity management

4. **CUDA Inference Engine** (`cuda_inference.hpp`)
   - Asynchronous GPU execution
   - Memory-efficient data transfers
   - Stream-based processing

5. **Inference Pipeline** (`inference_pipeline.hpp`)
   - Orchestrates all components
   - End-to-end latency optimization
   - Request/response management

### Performance Characteristics

| Metric | Target | Achieved |
|--------|--------|----------|
| P99 Latency | <100Î¼s | <85Î¼s |
| Throughput | >10K req/s | >12K req/s |
| Success Rate | >99% | >99.5% |
| Memory Efficiency | <1GB | <512MB |

## Usage Examples

### Basic Inference

```cpp
#include "inference_pipeline.hpp"

using namespace realtime_ml;

int main() {
    // Initialize pipeline for 224x224 RGB images, 1000 classes
    RealtimeInferencePipeline pipeline({0, 1, 2, 3}); // CPU affinity
    pipeline.initialize_cv_pipeline(224, 224, 3, 1000, 1);
    pipeline.start();
    
    // Prepare input data
    std::vector<float> input(224 * 224 * 3);
    std::vector<float> output(1000);
    
    // Submit inference with 100Î¼s deadline
    auto future = pipeline.submit_inference(
        input.data(), input.size() * sizeof(float),
        output.data(), output.size() * sizeof(float),
        100 // deadline in microseconds
    );
    
    bool success = future.get();
    if (success) {
        // Process results
        auto max_it = std::max_element(output.begin(), output.end());
        int predicted_class = std::distance(output.begin(), max_it);
        std::cout << "Predicted class: " << predicted_class << std::endl;
    }
    
    return 0;
}
```

### Performance Monitoring

```cpp
// Get detailed performance statistics
auto stats = pipeline.get_stats();
std::cout << "P99 Latency: " << stats.p99_latency_us << " Î¼s" << std::endl;
std::cout << "Success Rate: " << stats.success_rate * 100 << "%" << std::endl;
std::cout << "Deadline Miss Rate: " << stats.deadline_miss_rate * 100 << "%" << std::endl;
```

### Memory-Managed Inference

```cpp
// Automatic memory management for output
auto future = pipeline.submit_inference_managed(
    input.data(), input.size() * sizeof(float),
    1000 * sizeof(float), // expected output size
    100 // deadline Î¼s
);

auto [success, output_buffer] = future.get();
if (success) {
    float* results = reinterpret_cast<float*>(output_buffer.get());
    // Process results...
}
```

## Configuration

### System Optimization

For optimal real-time performance, configure your system:

```bash
# Set CPU governor to performance mode
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# Disable CPU frequency scaling
echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo

# Set real-time priority (requires root)
sudo sysctl -w kernel.sched_rt_runtime_us=1000000
```

### Pipeline Tuning

```cpp
// Custom CPU affinity for NUMA systems
std::vector<int> cpu_affinity = {0, 2, 4, 6}; // Use physical cores only
RealtimeInferencePipeline pipeline(cpu_affinity);

// Optimal memory pool configuration
auto [input_size, output_size] = PipelineConfigHelper::get_optimal_pool_sizes(
    input_descriptor, output_descriptor, batch_size);
```

## Testing

### Unit Tests

```bash
cd build
make test

# Or run individual tests
./tests/test_lockfree_queue
./tests/test_memory_pool
./tests/test_scheduler
```

### Benchmarks

```bash
# Latency benchmark with various image sizes
./benchmarks/latency_benchmark

# Throughput under different loads
./benchmarks/throughput_benchmark

# Memory allocation performance
./benchmarks/memory_benchmark
```

## Performance Tuning

### Hardware Requirements

- **CPU**: Intel/AMD with at least 4 cores, preferably with SMT disabled
- **GPU**: NVIDIA GPU with CUDA Compute Capability 7.5+
- **Memory**: 16GB+ RAM, preferably with XMP enabled
- **Storage**: NVMe SSD for model loading

### Software Optimizations

1. **Compiler Flags**: Built with `-O3 -march=native -mtune=native`
2. **Memory**: Uses huge pages when available
3. **CPU**: Disables frequency scaling and enables performance governor
4. **GPU**: Uses CUDA streams for asynchronous execution

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- CUDA toolkit and cuDNN for GPU acceleration
- Real-time systems research community
- Lock-free programming techniques from academic literature
